---
author: "Daniel Sherwood"
layout: post
title: "Designing Data Intesive Applications - Chapter 11"
date: 2025-06-02 15:44:00 -0500
categories: python
lead: "Stream Processing"
---
**I have had** the incredible opportunity to participate in the *Coder's Study Group* located in my city. 
The other participants are incredibly intelligent and experienced. 
Their insights have helped bring the abstract down into a more practical and relatable realm. 
We are now on Chapter 11 - Streaming Data. 
It is nearly the final chapter. Possibly, the author intends to tie many concepts introduced previously into this chatper.
Although I usually take notes privately, here I will put them out into the public space. 

# Streaming Data

The world is full of *unbounded* data -- that is, data which arrives gradually and continuously over time.
This data is ingested via *stream processing*.
This chapter:

1. Examines how streams are represented, stored, and transmitted over a network
2. Investigates the relationship between streams and databases
3. Explores approaches/tools for processing, with the intention of building applications.

## Transmitting Event Streams

In the streaming world, an individual record is referred to as an event, which usually has a timestamp associated with it.
An event is generated by a producer (publisher/sender) and then processed by a consumer (subscriber/recipient). 
Events are collected in *topics* or *streams*.

### Messaging Systems

In the Publisher/Subscriber model, what happens when: 

1. Producers send messages faster than consumers can process them?
2. Are messages lost when a nodes goes offline?

#### Direct Messaging from producers to consumers

One method is to pass messages from producers to consumers via UDP. 
This method is not fault tolerant. 

#### Message Brokers

Alternative is to use a database which is optimized for datastreams (also known as *message broker* / *message queue* ). 
It's a server, which producers & consumers connect as clients.
Helpful because the issue of durability is moved to the broker and away from producers/consumers.

#### Message Brokers Compared to Databases

Messages brokers can participate in two phase commit. 
But: 

1. They do not keep data - it is deleted when after successful deliver to consumer
2. If buffering is required ( longer than expected queue, then degradation of performance occurs).
3. They do notify clients when data changes, but do not support arbitrary queries

#### Multiple Consumers

Two patterns of reading messages: 

1. Load Balancing - Each message is delivered to *one* of the consumers. 
2. Fan Out - Each message is delivered to *all* of the consumers. 

#### Acknowledgements and redelivery 

A consumer must explicitly say that it has finished processing a message before the broker can delete it.
If this acknowledgement is not given and a timeout is reached, then the broker tries again with a different consumer. 
This can result in less than sequential ordering of messages. 

To avoid this issue, use a separate queue per consumer ( not use load balancing? )

### Partitioned Logs

A consumer only starts receiving messages after it has been activated, it has no knowledge of prior data. 
Now consider the idea of combining durable databases with low-latency notification facilities? 
Welcome to log-based message brokers.

Something something offsets. 

#### Using Logs For Message Storage

Logs are append only, possible to create a message broker that is a glorified log. 
A consumer receives messages by reading the log, sequentially.
( If a consumer reaches the end of the log, it waits for a notification that a new message has been appended...so that would 
be a separate service? )

Of course, logs can be partitioned on different machines for increased performance  ( but what about those notifications? ). 
I guess the individual partitions would serve a specific set of consumers.

#### Logs Compared to Traditional Messaging

Fan out is trivial to support with log based messaging ( each consumer can independently verify all logs ). 
But, as I thought, serving a particular partition to a set of consumers is also a thing. The downside is that the number of nodes sharing a topic can be at most the number of log partitions in that topic. 
If a single message is slow to process, that partition's performance is slowed due to blocking.

#### Consumer Offsets

Same process as in database replication, the log sequence number allows the message broker to act as the leader, 
and the consumer acts like a follower. 
When a node fails, a new node resumes where the old node left off, following the offset ( still could result in duplicated, processed messages. )

#### Disk Space Usage

Eventually the log will need to be over-written due to space limitations. 
So this entire setup is only a buffer.

#### When Consumers Cannot Keep Up with Producers

This isn't really a big problem, as there are other consumers ( presumably ) to take over. 

#### Replaying Old Messages

Can essentially switch a past set of messages by selecting previous offsets and feeding them back into the consumers.

## Databases and Streams

Apply concepts from streams and apply them to databases!
A stream is a log is a database, after all.

### Keeping Systems in Sync

No single system can provide for all computing needs. 
Thus, multiple systems are used, but they do need to be kept in sync.
Maybe the best way to do this is to have the search index be the follower and the database be the leader. 
Is this possible? 

### Change Data Capture

A problem is that the databases's replication logs are not considered a public API. 
Clients query the datamodel through via SQL, not parse through the replication logs. 
*Change Data Capture* is to observe all data changes written to a database and extract them in which they can be replicated to other systems. 

#### Implementing Change Data Capture



#### Initial Snapshot

#### Log Compaction 

#### API Support for Change Streams

### Event sourcing

#### Deriving Current State From the Event Log

#### Commands and Events

### State, Streams, and Immutability

(ooh a formula )

#### Advantages of Immutable Events

#### Deriving Several Views from the Same Event Log

#### Concurrency Control 

#### Limitations of Immutablity 

## Processing Streams

### Uses of Streams Processing

#### Complex Event Processing

#### Stream Analytics

#### Maintaining Materialized Views

#### Search on Streams

#### Message passing and RPC

### Reasoning about Time

#### Event Time Versus Processing Time

#### Knowing When You Are Ready 

#### Whose Clock Are You Using, Anyway?

#### Types of Windows

### Stream Joins

#### Stream-Stream Join (Window Join)

#### Stream-Table (Stream Enrichment)

#### Table-Table Join (Materialized View Maintenance)

#### Time-Dependence of Joins 

### Fault Tolerance

#### Microbatching and Checkpointing

#### Atomic Commit Revisited 

#### Idempotence

#### Rebuilding State After a Failure

## Summary
