---
author: "Daniel Sherwood"
layout: post
title: "Designing Data Intensive Applications - Chapter 11"
date: 2025-06-02 15:44:00 -0500
lead: "Stream Processing"
---
**I have had** the incredible opportunity to participate in the *Coder's Study Group* located in my city. 
The other participants are incredibly intelligent and experienced. 
Their insights have helped bring the abstract down into a more practical and relatable realm. 
We are now on Chapter 11 - Streaming Data. 
It is nearly the final chapter. Possibly, the author intends to tie many concepts introduced previously into this chatper.
Although I usually take notes privately, here I will put them out into the public space. 

# Streaming Data

The world is full of *unbounded* data -- that is, data which arrives gradually and continuously over time.
This data is ingested via *stream processing*.
This chapter:

1. Examines how streams are represented, stored, and transmitted over a network
2. Investigates the relationship between streams and databases
3. Explores approaches/tools for processing, with the intention of building applications.

## Transmitting Event Streams

In the streaming world, an individual record is referred to as an event, which usually has a timestamp associated with it.
An event is generated by a producer (publisher/sender) and then processed by a consumer (subscriber/recipient). 
Events are collected in *topics* or *streams*.

### Messaging Systems

In the Publisher/Subscriber model, what happens when: 

1. Producers send messages faster than consumers can process them?
2. Are messages lost when a nodes goes offline?

#### Direct Messaging from producers to consumers

One method is to pass messages from producers to consumers via UDP. 
This method is not fault tolerant. 

#### Message Brokers

Alternative is to use a database which is optimized for datastreams (also known as *message broker* / *message queue* ). 
It's a server, which producers & consumers connect as clients.
Helpful because the issue of durability is moved to the broker and away from producers/consumers.

#### Message Brokers Compared to Databases

Messages brokers can participate in two phase commit. 
But: 

1. They do not keep data - it is deleted when after successful deliver to consumer
2. If buffering is required ( longer than expected queue, then degradation of performance occurs).
3. They do notify clients when data changes, but do not support arbitrary queries

#### Multiple Consumers

Two patterns of reading messages: 

1. Load Balancing - Each message is delivered to *one* of the consumers. 
2. Fan Out - Each message is delivered to *all* of the consumers. 

#### Acknowledgements and redelivery 

A consumer must explicitly say that it has finished processing a message before the broker can delete it.
If this acknowledgement is not given and a timeout is reached, then the broker tries again with a different consumer. 
This can result in less than sequential ordering of messages. 

To avoid this issue, use a separate queue per consumer ( not use load balancing? )

### Partitioned Logs

A consumer only starts receiving messages after it has been activated, it has no knowledge of prior data. 
Now consider the idea of combining durable databases with low-latency notification facilities? 
Welcome to log-based message brokers.

Something something offsets. 

#### Using Logs For Message Storage

Logs are append only, possible to create a message broker that is a glorified log. 
A consumer receives messages by reading the log, sequentially.
( If a consumer reaches the end of the log, it waits for a notification that a new message has been appended...so that would 
be a separate service? )

Of course, logs can be partitioned on different machines for increased performance  ( but what about those notifications? ). 
I guess the individual partitions would serve a specific set of consumers.

#### Logs Compared to Traditional Messaging

Fan out is trivial to support with log based messaging ( each consumer can independently verify all logs ). 
But, as I thought, serving a particular partition to a set of consumers is also a thing. The downside is that the number of nodes sharing a topic can be at most the number of log partitions in that topic. 
If a single message is slow to process, that partition's performance is slowed due to blocking.

#### Consumer Offsets

Same process as in database replication, the log sequence number allows the message broker to act as the leader, 
and the consumer acts like a follower. 
When a node fails, a new node resumes where the old node left off, following the offset ( still could result in duplicated, processed messages. )

#### Disk Space Usage

Eventually the log will need to be over-written due to space limitations. 
So this entire setup is only a buffer.

#### When Consumers Cannot Keep Up with Producers

This isn't really a big problem, as there are other consumers ( presumably ) to take over. 

#### Replaying Old Messages

Can essentially switch a past set of messages by selecting previous offsets and feeding them back into the consumers.

## Databases and Streams

Apply concepts from streams and apply them to databases!
A stream is a log is a database, after all.

### Keeping Systems in Sync

No single system can provide for all computing needs. 
Thus, multiple systems are used, but they do need to be kept in sync.
Maybe the best way to do this is to have the search index be the follower and the database be the leader. 
Is this possible? 

### Change Data Capture

A problem is that the databases's replication logs are not considered a public API. 
Clients query the data model through via SQL, not parse through the replication logs. 
*Change Data Capture* is to observe all data changes written to a database and extract them in which they can be replicated to other systems. 

#### Implementing Change Data Capture

Whatever ends up consuming the log are called *derived data systems*. 
Essentially, one database is the leader and turns the others into followers. 
Triggers can be used to implement change data capture by registering triggers that observe all changes to data tables.
CDC is asynchronous ( which means replication lag applies ). 

#### Initial Snapshot

To get started, an initial snapshot is needed. 

#### Log Compaction 

Since the log files cannot grow past storage, use *log compaction* to keep things tidy. 
The storage engine looks for log records with the same key, destroys duplicates, and keeps only the most recent update for each key. 
Implemented for CDC, every change has a primary key, so the most recent write for that particular key is kept.

#### API Support for Change Streams

Databases are more and more supporting CDC and streams. 

### Event sourcing

Developed in the *domain-driven design* community, event sourcing stores all changes to the application state as a log of change events.
This essentially recording the users's actions as immutable events.

#### Deriving Current State From the Event Log

Applications using event sourcing need to take the log of events and transform it into application state that is suitable for showing to a user. 

#### Commands and Events

Commands are different from events. Any validation of a command  needs to happen synchronously, before it becomes an event. 
By the time the *consumer* sees the event, it has already become an immutable part of the log.

### State, Streams, and Immutability

Application state is the integration of an event stream in time.
Change stream is the differentiation state with respect to time.

#### Advantages of Immutable Events

Useful to have a log of immutable events when debugging or improving a system.

#### Deriving Several Views from the Same Event Log

Separating mutable state from the immutable event log, you can derive several different read-oriented representations from the same log of events. 

#### Concurrency Control 

Downside of event sourcing and CDC is that the consumers of the event log are asynchronous.
But deriving the current state from an event log also simplifies some aspects of concurrency control. 
An event can be designed to be a self-contained description of a user action. 
The user action would only require a single write in one place...which would make it atomic. 

#### Limitations of Immutablity 

There are times when it is necessary to absolutely delete data. This is surprisingly hard to accomplish. 

## Processing Streams

Three options for processing: 
1. Write data from event to a database
2. Push the event to a user
3. Push the event to another stream

This chapter will focus on #3. 
An operator that does takes input as read-only and makes output which is append-only.

### Uses of Streams Processing

Fraud detection, trading systems, manufacturing systems monitoring status, military and intelligence systems.

#### Complex Event Processing

*Complex Event Processing* allows you to specify rules to search for certain patterns of events in a stream. 
Queries are stored long term, and new events are evaluated against the query. 

#### Stream Analytics

Measuring rate, rolling average over some time period, comparing previous periods to current periods

#### Maintaining Materialized Views

Creating a view of the log of events. *huh*

#### Search on Streams

Text search on streams...again the query is stored and the data is run past the query. 

#### Message passing and RPC

RPC systems and message passing are not the same. Although, there is some crossover. 

### Reasoning about Time

Useful to include timestamps.

#### Event Time Versus Processing Time

Don't confuse event time and processing time! Sometimes the process takes longer than the event. 

#### Knowing When You Are Ready 

Have you received all events of a certain window?

#### Whose Clock Are You Using, Anyway?

To adjust for incorrect device clocks, log: 
1. Time at which the event occurred (device clock)
2. Time at which event was sent to the server (device clock)
3. Time at which the event was received by the server (server clock)

#### Types of Windows

1. Tumbling Window
2. Hopping Window
3. Sliding Window
4. Session Window

### Stream Joins

Because new events can appear anytime ona stream, joins to streams are more challenging than in batch jobs.

#### Stream-Stream Join (Window Join)

Stream processor needs to maintain *state*. An event is added to the appropriate index, and then the complementary stream's index is also checked. 
If there is match, emit an event declaring a match.

#### Stream-Table (Stream Enrichment)

Basically use CDC, stream processor subscribes to a changelog, which is then compared to a static table. 

#### Table-Table Join (Materialized View Maintenance)

ex.
```shell
SELECT 
    follows.follower_id as timeline_id, array_agg(tweets.* order by tweets.timestamp DESC)
FROM 
    tweets
JOIN 
    follows
        on follows.followee_id = tweets.sender_id
GROUP BY
    follows.follower_id 
```

#### Time-Dependence of Joins 

The above types require the stream processor to maintain some sort of state. 
Order is important, but there is no guarantee of ordering except within a partition.
( if state changes over time, and you join with state, which state do you choose? )

### Fault Tolerance

Difficult to handle faults because can never stop processing.

#### Microbatching and Checkpointing

Can break data into small blocks and treat each block as a miniature batch. 
That means you can rely on batching fault failure. 
This also creates a tumbling window.
Or maybe generate rolling checkpoints.

#### Atomic Commit Revisited 

Unclear exactly how atomic commit is implemented here.

#### Idempotence

This is an operation that can be applied multiple times ( but it has the same effect as applying it once ). 

#### Rebuilding State After a Failure

Can try to keep state in a remote datastore and replicate it. 
Or maybe keep state local to the stream processor and replicate it periodically. 
Or maybe rebuild thet state from the input streams ( ex if state conssits of aggregations ove a short window, it could easy enough to replay the input events within the window ).

## Summary

Finally!